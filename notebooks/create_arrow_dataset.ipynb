{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create arrow dataset\n",
    "\n",
    "The BrainLM processing function involves doing train-validation-test split.\n",
    "They hard coded to only use data with more than 200 time points, and there are a lot of the hard coded, unclear data scaling and normalisation. \n",
    "Need further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import argparse\n",
    "from math import ceil\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from importlib.resources import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"uk_biobank_dir\": \"../data/interim/brainlm_a424\",     # \"Path to directory containing dat files, A424 coordinates file, and A424 excel sheet.\",\n",
    "    \"arrow_dataset_save_directory\": \"../data/processed/brainlm_a424\",     # \"The directory where you want to save the output arrow datasets.\"\n",
    "    \"dataset_name\": \"test_data_arrow_norm\",\n",
    "}\n",
    "save_path = \"../data/processed/brainlm_a424\"\n",
    "\n",
    "ts_min_length = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's no A24 Coordinates dat file\n"
     ]
    }
   ],
   "source": [
    "all_dat_files = os.listdir(args[\"uk_biobank_dir\"])\n",
    "all_dat_files = [filename for filename in all_dat_files if \".dat\" in filename]\n",
    "try:\n",
    "    all_dat_files.remove(\"A424_Coordinates.dat\")\n",
    "    print('A424_Coordinates was removed from the list')\n",
    "except ValueError:\n",
    "    print(\"There's no A24 Coordinates dat file\")\n",
    "all_dat_files.sort()  # Sorted in ascending order, first 80% will be train. Assuming no bias in patient order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:06<00:00, 23.23it/s]\n"
     ]
    }
   ],
   "source": [
    "train_split_idx = len(all_dat_files)\n",
    "train_files = all_dat_files[:train_split_idx]\n",
    "sh_35 = 0\n",
    "sh_less = 0\n",
    "for idx,file in enumerate(tqdm(all_dat_files)):\n",
    "    try:\n",
    "        sample = np.loadtxt(os.path.join(args[\"uk_biobank_dir\"],file)).T #490, 424\n",
    "        if sample.shape[0] < ts_min_length:\n",
    "            print(sample.shape, idx, \"ommitted due to insufficient data\")\n",
    "            sh_less += 1\n",
    "        else:\n",
    "            sh_35 += 1\n",
    "        # print(sample.shape)\n",
    "    except UnicodeDecodeError:\n",
    "        print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:06<00:00, 23.60it/s]\n",
      "/home/hwang/simexp/hwang/hfplayground/.venv/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1620: RuntimeWarning: All-NaN slice encountered\n",
      "  return fnb._ureduce(a,\n"
     ]
    }
   ],
   "source": [
    "compute_Stats=True\n",
    "if compute_Stats:\n",
    "    num_files = sh_35 #len(all_dat_files_rs) + len(all_dat_files_tf)\n",
    "    all_stds = np.zeros([num_files, 424])\n",
    "    all_data = np.empty([num_files*ts_min_length, 424])\n",
    "    for idx,file in enumerate(tqdm(train_files)):\n",
    "        if idx == num_files:\n",
    "            break\n",
    "        # if idx%2000==0:\n",
    "        #     print('idx: {}, next file: {}'.format(idx,file))\n",
    "        try:\n",
    "            sample = np.loadtxt(os.path.join(args[\"uk_biobank_dir\"],file)) #490, 424\n",
    "            # print(sample.shape)\n",
    "        except UnicodeDecodeError:\n",
    "            print(file)\n",
    "        # sample = np.loadtxt(os.path.join(uk_biobank_dir_rs, file, 'rfMRI_REST','rfMRI_REST_Atlas_MSMAll_hp2000_clean_MGTR_zscored_HCP_MMP_BNAC.dat')).astype(np.float32).T\n",
    "        sample_mean = sample.mean(axis=0, keepdims=True)\n",
    "        sample_mean = sample_mean[None,:].repeat(sample.shape[0],1).squeeze()\n",
    "        sample = sample - sample_mean\n",
    "\n",
    "        idx_sample=idx\n",
    "\n",
    "\n",
    "        if sample.shape[0] < ts_min_length:\n",
    "            continue\n",
    "        try:\n",
    "            all_data[idx*ts_min_length:(idx+1)*ts_min_length,:] = sample[:ts_min_length,:]\n",
    "        except ValueError:\n",
    "            print(sample.shape)\n",
    "            print('idx: {}, idx_sample: {}'.format(idx,idx_sample))\n",
    "\n",
    "    global_std = np.std(all_data, axis=0)\n",
    "    data_median_per_voxel = np.median(all_data,axis=0)\n",
    "    data_mean_per_voxel = np.mean(all_data,axis=0)\n",
    "\n",
    "    all_data_nonzeros = np.copy(all_data)\n",
    "    all_data_nonzeros[all_data_nonzeros == 0] = 'nan'\n",
    "    quartiles = np.nanpercentile(all_data_nonzeros, [25, 75], axis=0)\n",
    "    IQR = quartiles[1,:]-quartiles[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting normalization stats: 100%|██████████| 155/155 [00:06<00:00, 23.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Normalization Calculations ---#\n",
    "# Calculate min and max value across train, validation, and test sets\n",
    "global_train_max = -1e9\n",
    "global_train_min = 1e9\n",
    "voxel_maximums_train = []\n",
    "voxel_minimums_train = []\n",
    "\n",
    "for filename in tqdm(train_files, desc=\"Getting normalization stats\"):\n",
    "    dat_arr = np.loadtxt(os.path.join(args[\"uk_biobank_dir\"], filename)).astype(\n",
    "        np.float32\n",
    "    ).T\n",
    "    #assert (\n",
    "    #    np.min(dat_arr) >= 0\n",
    "    #), \"Minimum of patient recording is a negative number, check normalization\"\n",
    "    if np.max(dat_arr) > global_train_max:\n",
    "        global_train_max = np.max(dat_arr)\n",
    "    if np.min(dat_arr) < global_train_min:\n",
    "        global_train_min = np.min(dat_arr)\n",
    "\n",
    "    dat_arr_max = np.max(dat_arr, axis=1)\n",
    "    dat_arr_min = np.min(dat_arr, axis=1)\n",
    "    voxel_maximums_train.append(dat_arr_max)\n",
    "    voxel_minimums_train.append(dat_arr_min)\n",
    "\n",
    "voxel_maximums_train = np.stack(voxel_maximums_train, axis=0)  # stack in time dimension\n",
    "voxel_minimums_train = np.stack(voxel_minimums_train, axis=0)\n",
    "global_per_voxel_train_max = np.max(voxel_maximums_train, axis=0)\n",
    "global_per_voxel_train_min = np.min(voxel_minimums_train, axis=0)\n",
    "\n",
    "# --- Convert All .dat Files to Arrow Datasets ---#\n",
    "# Training set\n",
    "train_dataset_dict = {\n",
    "    \"Raw_Recording\": [],\n",
    "    \"Voxelwise_RobustScaler_Normalized_Recording\": [],\n",
    "    \"All_Patient_All_Voxel_Normalized_Recording\": [],\n",
    "    \"Per_Patient_All_Voxel_Normalized_Recording\": [],\n",
    "    \"Per_Patient_Per_Voxel_Normalized_Recording\": [],\n",
    "    \"Per_Voxel_All_Patient_Normalized_Recording\": [],\n",
    "    \"Subtract_Mean_Normalized_Recording\": [],\n",
    "    \"Subtract_Mean_Divide_Global_STD_Normalized_Recording\": [],\n",
    "    \"Subtract_Mean_Divide_Global_99thPercent_Normalized_Recording\": [],\n",
    "    \"Filename\": [],\n",
    "    \"Patient ID\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing Data: 100%|██████████| 155/155 [00:08<00:00, 17.62it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fdc53140f041b0b0c055d9379796fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for filename in tqdm(train_files, desc=\"Normalizing Data\"):\n",
    "    dat_arr = np.loadtxt(os.path.join(args[\"uk_biobank_dir\"], filename)).astype(\n",
    "        np.float32\n",
    "    ).T\n",
    "\n",
    "    if dat_arr.shape[0] < ts_min_length:\n",
    "        continue\n",
    "\n",
    "    if dat_arr.shape[0] > 424:\n",
    "        dat_arr = dat_arr[:350, :]\n",
    "\n",
    "    global_norm_dat_arr = np.copy(dat_arr)\n",
    "    per_patient_all_voxels_norm_dat_arr = np.copy(dat_arr)\n",
    "    per_patient_per_voxel_norm_dat_arr = np.copy(dat_arr)\n",
    "    per_voxel_all_patient_norm_dat_arr = np.copy(dat_arr)\n",
    "    recording_mean_subtracted = np.copy(dat_arr)\n",
    "    recording_mean_subtracted2 = np.copy(dat_arr)\n",
    "    recording_mean_subtracted3 = np.copy(dat_arr.T)\n",
    "    global_std = 41.44047  # calculated in normalization notebook\n",
    "    _99th_percentile = 111.13143061224855  # calculated externally\n",
    "\n",
    "    # All patients, all voxels normalization\n",
    "    if (global_train_max - global_train_min) > 0.0:\n",
    "        global_norm_dat_arr = (global_norm_dat_arr - global_train_min) / (\n",
    "            global_train_max - global_train_min\n",
    "        )\n",
    "\n",
    "    # Per patient all voxel normalization\n",
    "    patient_all_voxel_min_val = np.min(per_patient_all_voxels_norm_dat_arr)\n",
    "    patient_all_voxel_max_val = np.max(per_patient_all_voxels_norm_dat_arr)\n",
    "    if (patient_all_voxel_max_val - patient_all_voxel_min_val) > 0.0:\n",
    "        per_patient_all_voxels_norm_dat_arr = (\n",
    "            per_patient_all_voxels_norm_dat_arr - patient_all_voxel_min_val\n",
    "        ) / (patient_all_voxel_max_val - patient_all_voxel_min_val)\n",
    "\n",
    "    # Per patient per voxel normalization\n",
    "    for voxel_idx in range(dat_arr.shape[1]):\n",
    "        patient_voxel_min_val = per_patient_per_voxel_norm_dat_arr[\n",
    "            :, voxel_idx\n",
    "        ].min()\n",
    "        patient_voxel_max_val = per_patient_per_voxel_norm_dat_arr[\n",
    "            :, voxel_idx\n",
    "        ].max()\n",
    "        if (patient_voxel_max_val - patient_voxel_min_val) > 0.0:\n",
    "            per_patient_per_voxel_norm_dat_arr[:, voxel_idx] = (\n",
    "                per_patient_per_voxel_norm_dat_arr[:, voxel_idx]\n",
    "                - patient_voxel_min_val\n",
    "            ) / (patient_voxel_max_val - patient_voxel_min_val)\n",
    "\n",
    "    # Per voxel all patient normalization\n",
    "    for voxel_idx in range(dat_arr.shape[1]):\n",
    "        voxel_maximum = global_per_voxel_train_max[voxel_idx]\n",
    "        voxel_minimum = global_per_voxel_train_min[voxel_idx]\n",
    "        if (voxel_maximum - voxel_minimum) > 0.0:\n",
    "            per_voxel_all_patient_norm_dat_arr[:, voxel_idx] = (\n",
    "                per_voxel_all_patient_norm_dat_arr[:, voxel_idx] - voxel_minimum\n",
    "            ) / (voxel_maximum - voxel_minimum)\n",
    "\n",
    "    # Subtract Mean, Scale by Global Standard Deviation normalization\n",
    "    for voxel_idx in range(dat_arr.shape[1]):\n",
    "        voxel_mean = recording_mean_subtracted[:, voxel_idx].mean()\n",
    "        recording_mean_subtracted[:, voxel_idx] = (\n",
    "            recording_mean_subtracted[:, voxel_idx] - voxel_mean\n",
    "        )\n",
    "\n",
    "    z_score_global_recording = np.divide(recording_mean_subtracted, global_std)\n",
    "\n",
    "    # Subtract Mean, Scale by global 99th percentile\n",
    "    for voxel_idx in range(dat_arr.shape[1]):\n",
    "        voxel_mean = recording_mean_subtracted2[:, voxel_idx].mean()\n",
    "        recording_mean_subtracted2[:, voxel_idx] = (\n",
    "            recording_mean_subtracted2[:, voxel_idx] - voxel_mean\n",
    "        )\n",
    "\n",
    "    #Voxelwise Robust Scaler Normalization\n",
    "    recording_mean_subtracted3 = recording_mean_subtracted3 - recording_mean_subtracted3.mean(axis=0)\n",
    "    recording_mean_subtracted3 = (recording_mean_subtracted3 - data_median_per_voxel / IQR)\n",
    "\n",
    "    _99th_global_recording = np.divide(recording_mean_subtracted2, _99th_percentile)\n",
    "\n",
    "    train_dataset_dict[\"Raw_Recording\"].append(dat_arr)\n",
    "    train_dataset_dict[\"Voxelwise_RobustScaler_Normalized_Recording\"].append(recording_mean_subtracted3)\n",
    "    train_dataset_dict[\"All_Patient_All_Voxel_Normalized_Recording\"].append(\n",
    "        global_norm_dat_arr\n",
    "    )\n",
    "    train_dataset_dict[\"Per_Patient_All_Voxel_Normalized_Recording\"].append(\n",
    "        per_patient_all_voxels_norm_dat_arr\n",
    "    )\n",
    "    train_dataset_dict[\"Per_Patient_Per_Voxel_Normalized_Recording\"].append(\n",
    "        per_patient_per_voxel_norm_dat_arr\n",
    "    )\n",
    "    train_dataset_dict[\"Per_Voxel_All_Patient_Normalized_Recording\"].append(\n",
    "        per_voxel_all_patient_norm_dat_arr\n",
    "    )\n",
    "    train_dataset_dict[\"Subtract_Mean_Normalized_Recording\"].append(\n",
    "        recording_mean_subtracted\n",
    "    )\n",
    "    train_dataset_dict[\n",
    "        \"Subtract_Mean_Divide_Global_STD_Normalized_Recording\"\n",
    "    ].append(z_score_global_recording)\n",
    "    train_dataset_dict[\n",
    "        \"Subtract_Mean_Divide_Global_99thPercent_Normalized_Recording\"\n",
    "    ].append(_99th_global_recording)\n",
    "    train_dataset_dict[\"Filename\"].append(filename)\n",
    "    train_dataset_dict[\"Patient ID\"].append(filename.split(\".dat\")[-1])\n",
    "\n",
    "arrow_train_dataset = Dataset.from_dict(train_dataset_dict)\n",
    "arrow_train_dataset.save_to_disk(\n",
    "    dataset_path=os.path.join(save_path, \"train\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65c7a35245b40179e0fecbb3130e61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/424 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# --- Save Brain Region Coordinates Into Another Arrow Dataset ---#\n",
    "coords_dat = np.loadtxt(files('hfplayground') / \"data/brainlm/atlases/A424_Coordinates.dat\").astype(np.float32)\n",
    "coords_pd = pd.DataFrame(coords_dat, columns=[\"Index\", \"X\", \"Y\", \"Z\"])\n",
    "coords_dataset = Dataset.from_pandas(coords_pd)\n",
    "coords_dataset.save_to_disk(\n",
    "    dataset_path=os.path.join(save_path, \"Brain_Region_Coordinates\")\n",
    ")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-76.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-74.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>420.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>421.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>-48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>422.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>-44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>423.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>424.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>-46.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index     X     Y     Z\n",
       "0      1.0  14.0 -78.0   4.0\n",
       "1      2.0  44.0 -64.0   4.0\n",
       "2      3.0  18.0 -76.0  32.0\n",
       "3      4.0  10.0 -74.0   0.0\n",
       "4      5.0  18.0 -92.0  16.0\n",
       "..     ...   ...   ...   ...\n",
       "419  420.0   0.0 -56.0 -38.0\n",
       "420  421.0   6.0 -52.0 -48.0\n",
       "421  422.0 -20.0 -36.0 -44.0\n",
       "422  423.0   0.0 -48.0 -36.0\n",
       "423  424.0  22.0 -38.0 -46.0\n",
       "\n",
       "[424 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
